---
layout: post
permalink: /01ead88355127f33e70117da2578556e80ac1dbc/:categories/:year/:month/:day/:title:output_ext
title:  "NLP in Japanese using BERT"
description: "NLP Question answering in Japanese using transformer's BERT"
type: card-dated
image: 
caption: 
categories: post
tags: 
  - NLP
  - BERT
author: Jose
card: card-2
---

## Introduction

I received a small robot that can sit, for example, on a reception desk and I thought 
to build a Bot that can answer the questions of customers.

So, in this NLP project, the task is to create a Bot that can answer questions based on
given information. The language to be used is **Japanese**.

The idea is to create a web service that receives the information text and the question made
by the customer. The model will load the text and the question and it will
output the answer. The answer will be sent to the robot that will speak
it aloud to the customer.

The robot already provides the tools to do speech-to-text and text-to-speech.
It also provides a framework to execute web service calls.

After searching for a good model that can handle this problem, I decided 
to use [Huggingface Transformers 2.10.0](https://huggingface.co/transformers/).
They have an explicit [section for SQuAD](https://huggingface.co/transformers/v2.10.0/examples.html#squad) 
using BERT model.

SQuAD is the Stanford Question Answering Dataset which 
is a reading comprehension dataset, consisting of questions posed by 
crowdworkers on a set of Wikipedia articles. The answer to every question is a segment of text.

Of course, the original SQuAD dataset is in English.
One of the most challenging things of this project is that the language 
is Japanese because there are few question-answering datasets in Japanese.
So let's begin by explaining how I created the dataset.

## Data preparation

There are few resources when dealing with Japanese in NLP. 

One of the best datasets I found was [NIILC Question Answering Dataset](https://mynlp.is.s.u-tokyo.ac.jp/niilc-qa/j_index.html) by Satoshi Sekine in 2003.
But it just contains around 800 questions with their answers that I could use.
These are scarce examples, so I kept searching.

Thanks to [NTCIR Project](http://research.nii.ac.jp/ntcir/index-ja.html) I also found the 
datasets [NTCIR-4 and NTCIR-5](http://research.nii.ac.jp/ntcir/data/data-ja.html).
They contain together more than 4000 thousand questions with their answers.

The next step was to create Python scripts to convert from their XML format 
to the expected SQuAD format.

For example, NIILC dataset has the following structure:
```xml
<?xml version="1.0" encoding="UTF-8"?>
<question id="NIILC-ECQA2015-00015-01">
   <text>日本では喫煙は何歳から可能なの?</text>
   <answers>
      <answer>20歳</answer>
   </answers>
   <meta>
      <E.1>Y</E.1>
      <E.2>C</E.2>
      <E.3>未成年者喫煙禁止法</E.3>
      <E.4>冒頭</E.4>
      <E.5>未成年者喫煙禁止法(みせいねんしゃきつえんきんしほう、明治33年3月7日法律第33号)は、満20歳未満の者[1]の喫煙禁止を目的とする法律である。</E.5>
      <E.6 />
      <E.7 />
      <E.8 />
      <E.memo />
   </meta>
</question>

```

But the required format was a `json` with the following structure:

```json
"title": "生活",
"paragraphs": [
    {
        "qas": [
            {
                "question": "日本では喫煙は何歳から可能なの?",
                "id": "NIILC-ECQA2015-00015-01",
                "answers": [
                    {
                        "text": "20歳",
                        "answer_start": 46
                    },
                ],
                "is_impossible": false
            }
        ],
        "context": "未成年者喫煙禁止法（みせいねんしゃきつえんきんしほう、明治33年3月7日法律第33号）は、満20歳未満の者[1]の喫煙禁止を目的とする法律である。"
    }
]
```

NTCIR datasets have different formats, so additional scripts were created for them.

## Model Development

The chosen model was [BERT](https://arxiv.org/pdf/1810.04805.pdf), which at the beginning of 
2020 was one of the best "small" NLP models. It makes use of bidirectional transformers. 

On top of that, the University of Kyoto offers 
some [pre-trained models](https://nlp.ist.i.kyoto-u.ac.jp/?ku_bert_japanese) 
based on that architecture.

Specifically, I used the [BASE Japanese_L-12_H-768_A-12_E-30_BPE_transformers](http://nlp.ist.i.kyoto-u.ac.jp/DLcounter/lime.cgi?down=http://lotus.kuee.kyoto-u.ac.jp/nl-resource/JapaneseBertPretrainedModel/Japanese_L-12_H-768_A-12_E-30_BPE_transformers.zip&name=Japanese_L-12_H-768_A-12_E-30_BPE_transformers.zip)
 (393MB) published 
on 2019/11/15.

#### First model

At first, I just used the NIILC dataset. This dataset was so small that the test
set was just 156 QAs. The model had an accuracy of **44%**. 

#### Following Models

In the following models, I used the NTCIR datasets.
I used NTCIR-4 Task 1 as a test set.
As a training set, I used NIILC, NTCIR-5, and NTCIR-4 Tasks 2 and 3.
In that case, the accuracy was boosted to 56% in the case of the NIILC test dataset and 
72% in the case of NTCIR-4 Task 1 test dataset.

After some fine tuning, the final accuracies were **58% for NIILC test dataset** 
and **76.8% in the case of NTCIR-4 Task 1 test dataset**.

The NIILC test dataset had 156 QAs and the NTCIR-4 Task 1 test dataset had 306 QAs.

## Conclusion

This was my first NLP project that I developed completely by myself other than the 
projects I did during the online courses.

For me, it seems that NLP tasks are harder than problems related to images,
but at the same time the effect that it feels like "magic" is the same.

I was very pleased with the accuracy of the model even though it was just around 77%.
Looking at the results, the model produced amazing answers to complex questions.

I also realized that evaluating the accuracy of such NLP models is hard, 
because the answer to a question might not be unique. 

For example, when asking "How long was the war?" The model may answer: "1971年から1974年" but
the correct answer was "1971年から1974年**まで**". The missing **まで** could produce a false miss.

I implemented some techniques to avoid that but after evaluating the 
results, still a few errors could be classified as actually non-errors.

On top of that, in Japanese, there are full and half-width characters.
It is not the same "１９７１" and "1971". 
However, this can be corrected easily and usually does not represent a problem.

Maybe if I had used a bigger model, the result could have improved a little bit,
but I think that the main problem was that the examples were scarce.

## Results

Here are some correct results produced by the model from the test set.

#### Example 1
```txt
Input context：
1914年6月28日、オーストリア＝ハンガリー帝国皇帝フランツ・ヨーゼフ1世の世継、フランツ・フェルディナント大公が、共同統治国ボスニア・ヘルツェゴヴィナ（英語版）（ボスニア）の首都サラエボで「青年ボスニア（英語版） (Mlada Bosna, ムラダ・ボスナ)」のボスニア系セルビア人（ボスニア語版）で民族主義者のガヴリロ・プリンツィプにより暗殺された。

Question：
第1次世界大戦が引き起こされた都市は?

Answer：
サラエボ
```

#### Example 2
```txt
Context：
通説では、「銃」に相当する鉄砲は天文12年(1543年)に、ポルトガル人をのせた中国の船舶が種子島に到着したことをもって伝来の最初とする。

Question：
日本に初めて鉄砲が伝来した場所は?

Answer：
種子島
```

#### Example 3
```txt
Context:
そんな中、A4ノートサイズ、2.7kgと軽量で、最小限のインターフェースを装備しながら、大型の液晶ディスプレイを備え、デスクトップタイプのパソコンと互換性を保持した製品として、1989年6月27日発表、同年7月に東芝から発売されたDynaBook(現・dynabook) J-3100SSは、19万8,000円という価格で衝撃を与えた。

Question:
初めてノート型パソコンを作ったメーカーは?

Answer:
東芝
```

#### Example 4
```txt
Context:
野党勢力は、得票でトップに立つ闘争民主党を中心に、選挙前に野党3連合を結成した国民信託党、国民覚せい党が結束し、他のイスラム系改革派の少数政党の正義党、月星党などとも連合を目指す「大野党連合」の実現が考えられる。

Question:
99年6月の総選挙を前に3党連合を結成したインドネシアの政党の名前は。

Answer:
闘争民主党
```

## Links

- [About SQuAD](https://rajpurkar.github.io/SQuAD-explorer/)
- [Article about BERT](https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270)
- [Kyoto University BERT models](https://nlp.ist.i.kyoto-u.ac.jp/?ku_bert_japanese)
- [Huggingface Transformers GitHub](https://github.com/huggingface/transformers)
- [Huggingface Transformers Documentation](https://huggingface.co/transformers/)
- [Huggingface SQuAD documentation](https://huggingface.co/transformers/v2.10.0/examples.html#squad)
- [NIILC QA dataset](https://mynlp.is.s.u-tokyo.ac.jp/niilc-qa/)
- [NTCIR datasets](http://research.nii.ac.jp/ntcir/data/data-ja.html)
