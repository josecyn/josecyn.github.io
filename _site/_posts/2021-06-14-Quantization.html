<!DOCTYPE html>

<html>
  <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<!--
  The above 2 meta tags *must* come first in the <head>
  to consistently ensure proper document rendering.
  Any other head element should come *after* these tags.
 -->
<title>Home</title>
<!-- Set the base URL for all relative URLs within the document -->
<base href="http://localhost:4000/">

<!--
  Allows control over where resources are loaded from.
  Place as early in the <head> as possible, as the tag  
  only applies to resources that are declared after it.
-->

<!-- Short description of the document (limit to 150 characters) -->
<!-- This content *may* be used as a part of search engine results. -->
<meta name="description" content="">

<!-- Control the behavior of search engine crawling and indexing -->
<meta name="robots" content="index,follow"><!-- All Search Engines -->
<meta name="googlebot" content="index,follow"><!-- Google Specific -->

<!-- Verify website ownership -->
<meta name="google-site-verification" content=""><!-- Google Search Console -->
<meta name="yandex-verification" content=""><!-- Yandex Webmasters -->
<meta name="msvalidate.01" content=""><!-- Bing Webmaster Center -->
<meta name="alexaVerifyID" content=""><!-- Alexa Console -->
<meta name="p:domain_verify" content=""><!-- Pinterest Console-->
<meta name="norton-safeweb-site-verification" content=""><!-- Norton Safe Web -->

<!-- Allows control over how referrer information is passed -->
<meta name="referrer" content="no-referrer">

<!-- Disable automatic detection and formatting of possible phone numbers -->
<meta name="format-detection" content="telephone=true">

<!-- Completely opt out of DNS prefetching by setting to "off" -->
<meta http-equiv="x-dns-prefetch-control" content="on">

<!-- Geo tags -->
<!-- meta name="ICBM" content="latitude, longitude"-->
<!-- meta name="geo.position" content="latitude;longitude"-->
<!-- meta name="geo.region" content="country[-state]"><Country code (ISO 3166-1): mandatory, state code (ISO 3166-2): optional; eg. content="US" / content="US-NY" -->
<!-- meta name="geo.placename" content="city/town"><eg. content="New York City" -->

<!-- Link to an external CSS file -->
<link href="https://fonts.googleapis.com/css2?family=Parisienne&family=Courgette&family=Lobster&family=Pacifico&family=Yellowtail&family=Noto+Sans+JP:wght@400;900&family=Poppins:wght@500&family=Righteous&display=swap" rel="stylesheet">

<link rel="stylesheet" href="/css/main.css">
<!-- BootStrap CDN --> 
<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/4.1.1/animate.min.css"/>
<script src="https://kit.fontawesome.com/32a2b2a489.js" crossorigin="anonymous"></script>

<script src="https://cdn.jsdelivr.net/npm/chart.js@2.8.0"></script> 

<!--  GIT TALK CDN  for comment system -->
<!--
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
<script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script> -->


<!-- Provides information about an author or another person -->
<link rel="me" href="mailto:josecarlosyn@gmail.com">
<link rel="me" href="sms:+">

<!-- Open Search -->
<!-- link rel="search" href="/open-search.xml" type="application/opensearchdescription+xml" title="Search Title" -->

<!-- Feeds -->
<!-- link rel="alternate" href="https://feeds.feedburner.com/example" type="application/rss+xml" title="RSS" -->
<!-- link rel="alternate" href="https://example.com/feed.atom" type="application/atom+xml" title="Atom 0.3" -->

<!-- For IE 10 and below -->
<!-- Place favicon.ico in the root directory - no tag necessary -->
<link rel="apple-touch-icon" sizes="180x180" href="/assets/img/icons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/assets/img/icons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/assets/img/icons/favicon-16x16.png">
<link rel="mask-icon" href="/assets/img/icons/safari-pinned-tab.svg" color="#5bbad5">

<meta property="fb:app_id" content="">
<meta property="og:url" content="/_posts/2021-06-14-Quantization.html">
<meta property="og:type" content="website">
<meta property="og:title" content="Pytorch Quantization applied to YOLOv3 and YOLOv3 tiny models.">
<meta property="og:image" content="http://placehold.it/750X300?text=Header+Image">
<meta property="og:image:alt" content="">
<meta property="og:description" content="Pytorch Quantization applied to YOLOv3 and YOLOv3 tiny models.">
<meta property="og:site_name" content="Pytorch Quantization Of YOLOv3 Family">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Jose">

<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:site" content="@">
<meta name="twitter:creator" content="@">
<meta name="twitter:url" content="/_posts/2021-06-14-Quantization.html">
<meta name="twitter:title" content="Pytorch Quantization applied to YOLOv3 and YOLOv3 tiny models.">
<meta name="twitter:description" content="Pytorch Quantization applied to YOLOv3 and YOLOv3 tiny models.">
<meta name="twitter:image" content="http://placehold.it/750X300?text=Header+Image">
<meta name="twitter:image:alt" content="">

<!-- Used for adding in-document CSS -->



<!-- JavaScript & No-JavaScript tags -->

<script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
<script src="https://code.jquery.com/jquery-3.5.1.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/js/bootstrap.bundle.min.js" integrity="sha384-ho+j7jyWK8fNQe+A12Hb8AhRq26LrZ/JpcUGGOn+Y7RsweNrtN/tE3MoK7ZeZDyx" crossorigin="anonymous"></script>
<!--
<script>
    $(document).ready(function() {
        $(".toast").toast('show');
    });
</script> -->

<script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js"></script>

<noscript>
  <!-- No JS alternative -->
</noscript>
  <body>
    
<nav class="navbar navbar-expand-lg glowing-border mb-4" style="font-family:Poppins;background-color:#10292e;">
  <a class="navbar-brand" href="/index.html"><h4 style="font-family:Noto Sans JP;font-weight:600;color:#149f98;">Home</h4></a>
  <button class="navbar-toggler text-black" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
    <i class="fas fa-caret-down"></i>
  </button>
  <div class="collapse navbar-collapse" id="navbarSupportedContent">
    <ul class="navbar-nav mr-auto">
        
              <li class="nav-item">
                  <a href="about.html" class="nav-link" style="font-family:Poppins;color:#149f98;">About</a>
              </li>
        
              <li class="nav-item">
                  <a href="access.html" class="nav-link" style="font-family:Poppins;color:#149f98;">Portfolio</a>
              </li>
        
              <li class="nav-item">
                  <a href="/assets/pdf/resume.pdf" class="nav-link" style="font-family:Poppins;color:#149f98;">Resume</a>
              </li>
        
              <li class="nav-item">
                  <a href="mailto:josecarlosyn@gmail.com" class="nav-link" style="font-family:Poppins;color:#149f98;">Contact me!</a>
              </li>
        
    </ul>
    <ul class="navbar-nav  justify-content-end">
      <li class="nav-item">
        
<!-- Displays Social Media Icons based on _config.yml settings -->
<span style="color:#CCCCCC;">
    <i> &nbsp;&nbsp; </i>
    
    
        <a href="https://github.com/josecyn">
        <span style="color: #CCCCCC;">
            <i class="fab fa-github-square fa-lg"></i></a>
        </span>
    
    
        <a href="https://www.linkedin.com/in/josecarlosyn">
        <span style="color: #CCCCCC;">
            <i class="fab fa-linkedin fa-lg"></i></a>
        </span>
    
    
    
    
     
    <i> &nbsp;&nbsp; </i>
</span>

      </li>
    </ul>
  </div>
</nav>

    <div class="container">
      
      <div class="card border-0">
          <div class="card-body">
            <p class="card-text">
                



<div class="blogcard card-2 mb-4 mt-4" style="width: 100%;">
  <div class="card-body">
    <h5 class="card-title text-center" style="font-size:20px;">Pytorch</h5>
    <h5 class="card-title text-center" style="font-size:36px;font-weight:700;">Pytorch Quantization Of YOLOv3 Family</h5>
    <h6 class="card-subtitle text-center" style="font-size:15px;">Author: Jose | Published 2021-06-14 20:19:00 +0900</h6>
    <div class="row mx-auto">
   <div class="col-md-4 mx-auto">
      <div class="card mb-4 mt-0 mb-0 border-0">
         <div class="card-body text-center">
            <div class="row mb-0">
               <div class="col border-right mb-0">
                  <a href="https://facebook.com/sharer/sharer.php?u=http://localhost:4000//_posts/2021-06-14-Quantization.html">
                  <i class="fab fa-facebook-f"></i>
                  <span class="text-muted" style="font-size:14px;">Share</span>
                  </a>
               </div>
               <div class="col border-right mb-0">
                  <a href="https://twitter.com/intent/tweet/?text=Pytorch Quantization Of YOLOv3 Family&amp;url=http://localhost:4000//_posts/2021-06-14-Quantization.html">
                  <i class="fab fa-twitter animate__animated animate__tada"></i>
                  <span class="text-muted" style="font-size:14px;">Tweet</span>
                  </a>
               </div>
               <div class="col border-right mb-0">
                  <a href="mailto:someone@yoursite.com?&subject=Pytorch Quantization Of YOLOv3 Family&body=Pytorch Quantization applied to YOLOv3 and YOLOv3 tiny models.">
                  <i class="fas fa-envelope"></i>
                  <span class="text-muted" style="font-size:14px;">Email</span>
                  </a>
               </div>
               <div class="col mb-0">
                  <a href="https://reddit.com/submit/?url=http://localhost:4000//_posts/2021-06-14-Quantization.html&amp;resubmit=true&amp;title=Pytorch Quantization Of YOLOv3 Family">
                  <i class="fab fa-reddit-alien"></i>
                  <span class="text-muted" style="font-size:14px;">Post</span>
                  </a>
               </div>
            </div>
         </div>
      </div>
   </div>
</div>
  </div>
</div>

<div class="card border-0" style="width: 100%;">
  <div class="card-body">
    <p class="card-text"><h2 id="introduction">Introduction</h2>

<p>In this document I will go through the process of quantize an existing YOLOv3 model using Pytorch.
Please note that nowadays, May 2021, Pytorch does not support quantization using GPU!</p>

<p>Quantization refers to convert input, weights and biases to floating point 16 or integer 8 bits.
Operations in floating point 32 bits have additional precision that isn’t needed for the model to have a high accuracy.
Because of that, the use of smaller precision increases execution speed while keeping a good accuracy.</p>

<p>However, when converting to integer 8, the impact is obviously bigger and some steps have to be taken in order to preserve the model’s accuracy.</p>

<p>In this guide we will take a look at those steps. We will focus on int8 because that is
where you can decrease the execution time the most.</p>

<h2 id="first-steps">First Steps</h2>

<p>The idea of quantization is to convert the input of the model to int8, execute the forward pass
and convert the result back to floating point.
This approach is fine, but sometimes the model may contain operations that do not support int8 input.</p>

<p>Check the following link to have a look at the <a href="https://pytorch.org/docs/stable/quantization-support.html">supported operators</a>.</p>

<p>If you use a simple model such as MobileNet or even ResNet, all operators are supported and you don’t have to change the code that much.</p>

<p>As stated in the introduction, nowadays, Pytorch only support quantization using CPU. Because of that, you have to
load or move the model to the GPU.</p>

<p>In the case of YOLOv3 and YOLOv3-tiny I found that I was using <code class="language-plaintext highlighter-rouge">leakyRELU</code> which is not supported.
So I replaced the <code class="language-plaintext highlighter-rouge">leakyRELU</code> by plain <code class="language-plaintext highlighter-rouge">RELU</code>.
After this replacement, I retrained the model for a few epochs just to adapt to the new component.
This is an important step! Afterwards, check the model’s accuracy, it should not have changed that much.</p>

<p>Of course, there may be other operators that are not supported and can’t be changed so easily.
In the case of YOLOv3 you can see that the YOLO layer is, of course, not supported.
Moreover, in the case of the YOLOv3-tiny version I used, the operation <code class="language-plaintext highlighter-rouge">ZeroPad</code>, which is not supported.</p>

<p>Let’s see in the next chapter how can we deal with such cases.</p>

<h2 id="unsupported-operations-what-to-do">Unsupported operations, what to do</h2>

<p>Other than replacing the operator with a similar one like we did with <code class="language-plaintext highlighter-rouge">leakyRELU</code> and <code class="language-plaintext highlighter-rouge">RELU</code>,
there is another possibility, but first we need to know how quantization internally works.</p>

<p>When we quantize weights, biases and operators, we switch from fp32 data to int8 data.
When executing the <code class="language-plaintext highlighter-rouge">forward</code> pass, if one operator expects fp32 data and receives int8 data, it will fail and vice versa will also fail.
That’s why Pytorch offers the possibility to convert data from/to int8 and fp32.
The name of functions for that purpose are: <code class="language-plaintext highlighter-rouge">DeQuantStub</code> and <code class="language-plaintext highlighter-rouge">QuantStub</code>.</p>

<p>So, if we have an unsupported operator, the only thing you need to do is convert its input data from int8 to fp32.
Of course, this conversion takes time and memory so, in terms of performance, the least changes the better.</p>

<p>To have a clear view, it would be something like this:</p>

<p><img src="/assets/img/posts/quantization/dequantstub_quantstub.png" alt="Iterative forward" /></p>

<p>Note that between supported operations you don’t need to change the data type.</p>

<p>Let’s see an example of how to control this flow programmatically.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">quant</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">quantization</span><span class="p">.</span><span class="n">QuantStub</span><span class="p">()</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">dequant</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">quantization</span><span class="p">.</span><span class="n">DeQuantStub</span><span class="p">()</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">module_list</span> <span class="o">=</span> <span class="c1"># create model
</span>    <span class="p">...</span>


<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">yolo_outputs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">noquant_layers</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">get_noquant_layers</span><span class="p">()</span>        <span class="c1"># get no quant operations index
</span>
    <span class="n">x_isquant</span> <span class="o">=</span> <span class="bp">False</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">module_list</span><span class="p">):</span>
        <span class="c1"># quant control
</span>        <span class="k">if</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">noquant_layers</span> <span class="ow">and</span> <span class="n">x_isquant</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">dequant</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">x_isquant</span> <span class="o">=</span> <span class="bp">False</span>
        <span class="k">elif</span> <span class="n">i</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">noquant_layers</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">x_isquant</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">quant</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">x_isquant</span> <span class="o">=</span> <span class="bp">True</span>

        <span class="n">x</span> <span class="o">=</span> <span class="n">module</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div>

<p>In this example, I get the “no quant layers” at the beginning calling the method <code class="language-plaintext highlighter-rouge">self.get_noquant_layers()</code>.
It returns a list with the unsupported layers index.
Those layers need fp32 input, so if the data is in int8 format, we need to convert it.
That’s why I added a “quant control” block.
The quant control block checks whether the module that’s about to be executed needs fp32 or not.
In case the module needs fp32 input, and the input is int8 type, we convert it.
In case the module needs int8 and the data is of type fp32, we also convert it back to int8.</p>

<p>For those conversions, You have to create 2 properties in the model constructor <code class="language-plaintext highlighter-rouge">__init__(self)</code>.
Note the <code class="language-plaintext highlighter-rouge">self.quant()</code> and <code class="language-plaintext highlighter-rouge">self.dequant()</code> methods.</p>

<h2 id="fusing-operations">Fusing Operations</h2>

<p>Now, that the forward flow is correct, let’s take a step further.
Pytorch has a few already optimized operators that include multiple operations inside just one operation.
For example <code class="language-plaintext highlighter-rouge">ConvReLU2d</code> includes <code class="language-plaintext highlighter-rouge">convolution + relu</code> operations.
You can find these operations in this <a href="https://pytorch.org/docs/stable/quantization-support.html#torch-nn-intrinsic">list</a>.</p>

<p>So, using these combined operations we can extract a little bit more of performance.
Pytorch provides a function for that task that is called <code class="language-plaintext highlighter-rouge">torch.quantization.fuse_modules()</code>.
This is an example on how to use it:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">fuse_model</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">module_list</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">m</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">torch</span><span class="p">.</span><span class="n">quantization</span><span class="p">.</span><span class="n">fuse_modules</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="p">[[</span><span class="s">'conv'</span><span class="p">,</span> <span class="s">'batch_norm'</span><span class="p">,</span> <span class="s">'relu'</span><span class="p">]],</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<p>In the code above, we go through all the sequential modules checking whether the first element is a convolution layer.
If that is the case, we call the Pytorch method with the keys we use in our model <code class="language-plaintext highlighter-rouge">['conv', 'batch_norm', 'relu']</code>.
Pytorch will take those operations and merge them in a single one such as <code class="language-plaintext highlighter-rouge">ConvReLU2d</code> (Conv2d + ReLU).</p>

<p>Example before fusing:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(0): Sequential(
      (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (batch_norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU()
    )
</code></pre></div></div>

<p>After fusing:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(0): Sequential(
      (conv): ConvReLU2d(
        (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): ReLU()
      )
      (batch_norm): Identity()
      (relu): Identity()
    )
</code></pre></div></div>

<p>Note the use of the <code class="language-plaintext highlighter-rouge">Identity()</code> operation in the replaced operators <code class="language-plaintext highlighter-rouge">batch_norm</code> and <code class="language-plaintext highlighter-rouge">relu</code>.</p>

<p>After fusing those operators, execute a validation on the model to check that the accuracy didn’t decrease.
The model accuracy should be the same.</p>

<h2 id="what-to-do-with-skip-connections">What To Do With Skip Connections</h2>

<p>In <code class="language-plaintext highlighter-rouge">ResNet</code> networks and many other architectures, we may use skip connections. In YOLOv3 architecture we do, and it needs to be handled.
If you just concatenate 2 tensors using <code class="language-plaintext highlighter-rouge">torch.cat()</code> and both tensors have the same datatype (int8 or fp32) it should be fine.
However, when skipping connections the addition <code class="language-plaintext highlighter-rouge">+</code> operator is often used.
In that case, we need to update this operator to work with <code class="language-plaintext highlighter-rouge">int8</code> input.</p>

<p>To achieve that, you have to declare a new property in your class like this:</p>

<p><code class="language-plaintext highlighter-rouge">self.f_add = nn.quantized.FloatFunctional()</code></p>

<p>It has to be at class level because this operation will be calibrated so it has to save statistics. It can’t be created ‘on the fly’.</p>

<p>When executing the addition operation, you can use it like this:</p>

<p><code class="language-plaintext highlighter-rouge">x = self.f_add.add(x, previous_x)</code></p>

<p>Of course you have to pay attention that the datatypes of the tensors <code class="language-plaintext highlighter-rouge">x</code> and <code class="language-plaintext highlighter-rouge">previous_x</code> match!</p>

<p>This is how it looks when the model structure is printed:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(17): Connect(
  (f_add): FloatFunctional(
    (activation_post_process): Identity()
  )
)
</code></pre></div></div>

<h2 id="different-quantization-methods">Different Quantization Methods</h2>

<p>Once the model is ready, we can start the process of post-training quantization.</p>

<p>Let’s see 2 options:</p>

<p>The <strong>post-training static quantization</strong> method, involves not just converting the weights from <code class="language-plaintext highlighter-rouge">float</code> to <code class="language-plaintext highlighter-rouge">int</code>, as in dynamic quantization,
but also performing the additional step of first feeding batches of data through the network and
computing the resulting distributions of the different activations.</p>

<p><strong>Post-training quantization-aware training (QAT)</strong> is the quantization method that typically results in the <strong>highest accuracy</strong>.
With QAT, all weights and activations are “fake quantized” during both the forward and backward passes of training:
that is, <code class="language-plaintext highlighter-rouge">float</code> values are rounded to mimic <code class="language-plaintext highlighter-rouge">int8</code> values, but all computations are still done with floating point numbers.
Thus, all the weight adjustments during training are made while “aware” of the fact that the model will ultimately be quantized;
after quantizing, therefore, this method will usually yield higher accuracy than either dynamic quantization or post-training static quantization.</p>

<h2 id="how-to-set-the-quantization-configuration">How To Set The Quantization Configuration</h2>

<p>There are currently 2 different configurations for the quantization process:
<code class="language-plaintext highlighter-rouge">fbgemm</code> (for use on x86, https://github.com/pytorch/FBGEMM) and <code class="language-plaintext highlighter-rouge">qnnpack</code> (for use on the ARM QNNPACK library https://github.com/pytorch/QNNPACK).</p>

<p>In this case, I am targeting ARM processors, so the best configuration to choose should be <code class="language-plaintext highlighter-rouge">qnnpack</code>.
However, I would recommend you to try both methods.</p>

<p>The code to set the configuration and prepare the model couldn’t be easier, but it depends on the quantization method:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>qconfig = torch.quantization.get_default_qconfig('qnnpack')
torch.quantization.prepare(model, inplace=True)
</code></pre></div></div>

<p>However, if you are using <strong>quantization aware training</strong> method, you have to call this function:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span><span class="p">.</span><span class="n">qconfig</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">quantization</span><span class="p">.</span><span class="n">get_default_qat_qconfig</span><span class="p">(</span><span class="s">'qnnpack'</span><span class="p">)</span>
<span class="n">torch</span><span class="p">.</span><span class="n">quantization</span><span class="p">.</span><span class="n">prepare_qat</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<p>When printing the configuration, the output looks like this (QAT training):</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>QConfig(activation=functools.partial(&lt;class 'torch.quantization.fake_quantize.FakeQuantize'&gt;, observer=&lt;class 'torch.quantization.observer.MovingAverageMinMaxObserver'&gt;, quant_min=0, quant_max=255, reduce_range=False), weight=functools.partial(&lt;class 'torch.quantization.fake_quantize.FakeQuantize'&gt;, observer=&lt;class 'torch.quantization.observer.MovingAverageMinMaxObserver'&gt;, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, reduce_range=False))
</code></pre></div></div>

<p>If you want to check manually if the observers were correctly placed, you can just print the module like this:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">module_list</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>
</code></pre></div></div>

<p>It prints the information of the first element in the first layer (static quantization).</p>

<p>Output:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ConvReLU2d(
 (0): Conv2d(
   3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
   (activation_post_process): HistogramObserver()
 )
 (1): ReLU(
   (activation_post_process): HistogramObserver()
 )
)
</code></pre></div></div>

<p>Note the <code class="language-plaintext highlighter-rouge">HistogramObserver()</code> functions.</p>

<p>And this is how it looks for QAT:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> ConvReLU2d(
  3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
  (activation_post_process): FakeQuantize(
    fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), scale=tensor([1.]), zero_point=tensor([0])
    (activation_post_process): MovingAverageMinMaxObserver(min_val=tensor([]), max_val=tensor([]))
  )
  (weight_fake_quant): FakeQuantize(
    fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), scale=tensor([1.]), zero_point=tensor([0])
    (activation_post_process): MovingAverageMinMaxObserver(min_val=tensor([]), max_val=tensor([]))
  )
)
</code></pre></div></div>

<p>Note the <code class="language-plaintext highlighter-rouge">FakeQuantize()</code> functions.</p>

<h2 id="post-training-quantization-general-steps">Post-training Quantization General Steps</h2>

<p>basic steps to perform post-training static quantization:</p>

<ol>
  <li>Load model</li>
  <li>Evaluate the model to get the baseline accuracy</li>
  <li>Fuse possible operations (for example: conv, bn and relu)</li>
  <li>[Optional] check the baseline accuracy again (should be the same as in step 2)</li>
  <li>Set quantization configuration</li>
  <li>Calibrate / QAT train</li>
  <li>Convert the model to quantized model</li>
  <li>Evaluate the model to check accuracy regarding baseline</li>
  <li>Save the quantized model</li>
</ol>

<p>In the step 6, depending on your quantization method, you will calibrate or do QAT training.
Read the following sections to understand the difference between them.</p>

<h3 id="post-training-static-quantization-calibration">Post-Training Static Quantization Calibration</h3>

<p>In the process of calibration, we set the model in <code class="language-plaintext highlighter-rouge">eval()</code> mode and execute inference using training data.
Pytorch captures the distributions of the activations automatically.</p>

<p>In the calibration process you can use a subset or the whole training set. And you can even do multiple epochs.
I would test different options and compare the results.</p>

<p>So, there’s nothing else to add about the calibration step, you just have to do inference. Just keep in
mind to use a representative set of data of the distribution of your dataset.</p>

<p>Let’s see how the quantization-aware-training differs from calibration in the next section.</p>

<h3 id="post-training-quantization-aware-training">Post-training Quantization Aware Training</h3>

<p>In static quantization we did a calibration step where we executed inference.
In QAT training, you just have to train the model as you would normally for a few epochs.
During forward and backward Pytorch will collect statistics that will use later during quantization.
Additionally, you can also do some tricks to try to increase accuracy:</p>

<ul>
  <li>Switch batch norm to use running mean and variance towards the end of training to better match inference numerics.</li>
  <li>We also freeze the quantizer parameters (scale and zero-point) and fine tune the weights.</li>
</ul>

<p>Example of training loop:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">args</span><span class="p">.</span><span class="n">cal_epochs</span><span class="p">):</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'QAT Training process epoch </span><span class="si">{</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s">/</span><span class="si">{</span><span class="n">args</span><span class="p">.</span><span class="n">cal_epochs</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">opt</span><span class="p">,</span> <span class="n">conf</span><span class="p">,</span> <span class="n">data_loaders</span><span class="p">[</span><span class="s">'train'</span><span class="p">])</span>    <span class="c1"># training epoch
</span>    <span class="k">if</span> <span class="n">epoch</span> <span class="o">&gt;</span> <span class="mi">3</span><span class="p">:</span>
        <span class="c1"># Freeze quantizer parameters
</span>        <span class="n">model</span><span class="p">.</span><span class="nb">apply</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">quantization</span><span class="p">.</span><span class="n">disable_observer</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">epoch</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">:</span>
        <span class="c1"># Freeze batch norm mean and variance estimates
</span>        <span class="n">model</span><span class="p">.</span><span class="nb">apply</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">intrinsic</span><span class="p">.</span><span class="n">qat</span><span class="p">.</span><span class="n">freeze_bn_stats</span><span class="p">)</span>

    <span class="c1"># Check the accuracy after each epoch
</span>    <span class="n">quantized_model</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">quantization</span><span class="p">.</span><span class="n">convert</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nb">eval</span><span class="p">(),</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="n">metrics</span> <span class="o">=</span> <span class="n">test</span><span class="p">(</span><span class="n">quantized_model</span><span class="p">.</span><span class="nb">eval</span><span class="p">(),</span> <span class="n">args</span><span class="p">,</span> <span class="n">loaders</span><span class="p">[</span><span class="s">'test'</span><span class="p">])</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"## QAT Model Evaluation Results Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s">##"</span><span class="p">)</span>
    <span class="n">pprint</span><span class="p">.</span><span class="n">pprint</span><span class="p">(</span><span class="n">metrics</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">'QAT Training done'</span><span class="p">)</span>
</code></pre></div></div>

<p>Again, I encourage you to test different parameters.
Please note that this process can be veeeeery slow for big datasets or many epochs.</p>

<h2 id="save-model">Save model</h2>

<p>When saving for using in a mobile device, for example, you have to do it in <code class="language-plaintext highlighter-rouge">jit</code> format.
For example:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_traced_module</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">img_size</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">:</span>
    <span class="n">input_data</span> <span class="o">=</span> <span class="n">get_random_input</span><span class="p">(</span><span class="n">img_size</span><span class="p">,</span> <span class="n">model</span><span class="p">.</span><span class="n">gpu</span><span class="p">)</span>      <span class="c1"># get_random_input just uses torch.randn
</span>    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">jit</span><span class="p">.</span><span class="n">trace</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">forward</span><span class="p">,</span> <span class="n">input_data</span><span class="p">).</span><span class="nb">eval</span><span class="p">()</span>

<span class="n">torch</span><span class="p">.</span><span class="n">jit</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="n">get_traced_module</span><span class="p">(</span><span class="n">quantized_model</span><span class="p">,</span> <span class="n">imsize</span><span class="p">),</span> <span class="n">args</span><span class="p">.</span><span class="n">export_file</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="summary-whole-process-step-by-step">Summary Whole Process Step-by-Step</h2>

<ol>
  <li>Train a floating point model or load a pre-trained floating point model.</li>
  <li>Move the model to CPU and switch model to evaluation mode.</li>
  <li>Apply layer fusion and check if the layer fusion results in correct model.</li>
  <li>Apply torch.quantization.QuantStub() and torch.quantization.QuantStub() to the inputs and outputs, respectively.</li>
  <li>Specify quantization configurations, such as symmetric quantization or asymmetric quantization, etc.</li>
  <li>Prepare quantization model for post-training calibration.</li>
  <li>Run post-training calibration.</li>
  <li>Convert the calibrated floating point model to quantized integer model.</li>
  <li>[Optional] Verify accuracies and inference performance gain.</li>
  <li>Save the quantized integer model.</li>
</ol>

<h2 id="mobile-optimization">Mobile Optimization</h2>

<p>Apart from using <code class="language-plaintext highlighter-rouge">qnnpack</code> configuration, you can also take a few measures.
I particularly like this link: https://pytorch.org/tutorials/recipes/mobile_perf.html</p>

<p>Check it out!</p>

<h2 id="conclusion">Conclusion</h2>

<p>Using quantization techniques you can reduce the model’s execution time while
having a close accuracy to the baseline. Execution speed can be 2x-4x faster (depending on
multiple factors) and the model size will be reduced too, because the weights are stored directly in int8
taking less space in your memory.</p>

<h2 id="links">Links</h2>

<ul>
  <li>https://pytorch.org/tutorials/recipes/mobile_perf.html</li>
  <li>https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html</li>
  <li>https://pytorch.org/docs/stable/quantization.html</li>
  <li>https://leimao.github.io/blog/PyTorch-Static-Quantization/</li>
  <li>https://pytorch.org/docs/stable/quantization-support.html</li>
</ul>

</p>
  </div>
</div>


            </p>
            
              <p class="card-text"><small class="text-muted">This page was last updated at 2021-06-15 16:38.</small></p>
            
          </div>
      </div>
      
    </div>
    <div class="navbar navbar-default navbar-fixed-bottom">
  <div class="container">
    <p class="navbar-text pull-left">© 2020 - Theme Built By Tyler Butler, See it on
        <a href="https://github.com/tcbutler320/Jekyll-Theme-Dumbarton" target="_blank" >GitHub</a>
    </p>
    <p class="navbar-text pull-right">
            <!-- Displays Social Media Icons based on _config.yml settings -->

<span style="color: Black;">
     
    
        <a href="https://github.com/josecyn">
        <span style="color: black;">
            <i class="fab fa-github-square fa-lg"></i></a>
        </span>
    
    
        <a href="https://www.linkedin.com/in/josecarlosyn">
        <span style="color: black;">
            <i class="fab fa-linkedin fa-lg"></i></a>
        </span>
    
    
    
    
     
    <i> &nbsp;&nbsp; </i>
</span>

    </p>
  </div>
</div> 

<script>
  $(function () {
    $('[data-toggle="popover"]').popover()
  })
</script>
  </body>
</html>